# ğŸƒ ×”×’×“×¨×ª musicRay ×¢×œ RunPod

## ××“×¨×™×š ××”×™×¨ ×œ×¤×¨×™×¡×” ×¢×œ RunPod

### 1. ×™×¦×™×¨×ª Pod

1. ×”×™×›× ×¡ ×œ-[RunPod.io](https://runpod.io)
2. ×œ×—×¥ ×¢×œ "Deploy" â†’ "GPU Pod"
3. ×‘×—×¨ GPU ×—×–×§:
   - **××•××œ×¥**: RTX 4090 (24GB VRAM) - ××”×™×¨ ×‘×™×•×ª×¨
   - **××œ×˜×¨× ×˜×™×‘×”**: RTX 3090 (24GB VRAM) - ×˜×•×‘ ×•×–×•×œ ×™×•×ª×¨
   - **××™× ×™××•×**: RTX 3080 (10GB VRAM) - ×¢×•×‘×“ ××‘×œ ×™×•×ª×¨ ××™×˜×™

### 2. ×”×’×“×¨×ª Container

```bash
# Image: runpod/pytorch:2.1.0-py3.10-cuda12.1.1-devel-ubuntu22.04
# Container Disk: 20GB (××™× ×™××•×)
# Volume: ×œ× × ×“×¨×© ×œ××˜×¨×•×ª ×¤×™×ª×•×—
```

### 3. ×¤×¨×™×¡×ª ×”×§×•×“

#### ××•×¤×¦×™×” ×': ×”×¢×œ××” ×™×©×™×¨×”
```bash
# ×”×ª×—×‘×¨ ×œ-Pod ×“×¨×š SSH ××• Web Terminal
cd /workspace

# ×©×›×¤×•×œ ×”×¤×¨×•×™×§×˜
git clone https://github.com/your-username/musicRay.git
cd musicRay/backend

# ×”×ª×§× ×ª ×ª×œ×•×™×•×ª
pip install -r requirements-gpu.txt

# ×‘×“×™×§×ª ×ª×§×™× ×•×ª
python test_setup.py

# ×”×¨×¦×”
uvicorn app:app --host 0.0.0.0 --port 8000
```

#### ××•×¤×¦×™×” ×‘': Docker Build
```bash
cd /workspace
git clone https://github.com/your-username/musicRay.git
cd musicRay

# ×‘× ×™×™×ª Docker image
docker build -f backend/Dockerfile.runpod -t musicray-runpod backend/

# ×”×¨×¦×”
docker run --gpus all -p 8000:8000 -e RUNPOD_POD_ID=$RUNPOD_POD_ID musicray-runpod
```

### 4. ×—×™×‘×•×¨ Frontend

#### ××•×¤×¦×™×” ×': Frontend ××§×•××™
```bash
# ×‘××—×©×‘ ×”××§×•××™
cd frontend
npm install

# ×¢×“×›×Ÿ ××ª ×”-API URL ×‘-.env.local
echo "NEXT_PUBLIC_API_URL=https://your-pod-id-8000.proxy.runpod.net" > .env.local

# ×”×¨×¥
npm run dev
```

#### ××•×¤×¦×™×” ×‘': Frontend ×’× ×¢×œ RunPod
```bash
# ×‘-Pod
cd /workspace/musicRay/frontend
npm install
npm run build

# ×”×’×“×¨ proxy ×œ-port 3000
npm start
```

### 5. URLs ×•×’×™×©×”

```bash
# Backend API
https://your-pod-id-8000.proxy.runpod.net

# Frontend (×× ×¨×¥ ×¢×œ RunPod)  
https://your-pod-id-3000.proxy.runpod.net

# SSH Access
ssh root@your-pod-id.proxy.runpod.net -p 22
```

## âš¡ ××•×¤×˜×™××™×–×¦×™×•×ª ×œRunPod

### ×–×™×›×¨×•×Ÿ GPU
```python
# ×”×’×“×¨×•×ª ××•×ª×××•×ª ×‘-separate.py
if IS_RUNPOD:
    # ×¤×¨××˜×¨×™× ×œ× ×™×¦×•×œ ××§×¡×™××œ×™ ×©×œ GPU
    --shifts 10      # ××™×›×•×ª ××§×¡×™××œ×™×ª
    --segment 16     # ×¤×œ×—×™× ×’×“×•×œ×™×
    --overlap 0.5    # ×—×¤×™×¤×” ××§×¡×™××œ×™×ª
```

### ×‘×™×¦×•×¢×™×
- **RTX 4090**: ~2-3 ×“×§×•×ª ×œ×©×™×¨ ×©×œ 4 ×“×§×•×ª
- **RTX 3090**: ~3-4 ×“×§×•×ª ×œ×©×™×¨ ×©×œ 4 ×“×§×•×ª  
- **RTX 3080**: ~4-6 ×“×§×•×ª ×œ×©×™×¨ ×©×œ 4 ×“×§×•×ª

### ×¢×œ×•×™×•×ª (×œ×©×¢×”)
- **RTX 4090**: ~$0.80/hour
- **RTX 3090**: ~$0.50/hour
- **RTX 3080**: ~$0.40/hour

## ğŸ”§ ×‘×“×™×§×ª ×ª×§×™× ×•×ª

```bash
# ×‘×“×™×§×ª GPU
nvidia-smi

# ×‘×“×™×§×ª PyTorch + CUDA
python -c "import torch; print(f'CUDA: {torch.cuda.is_available()}, GPU: {torch.cuda.get_device_name()}')"

# ×‘×“×™×§×ª Demucs
python -c "from demucs.api import Separator; print('Demucs OK')"

# ×‘×“×™×§×ª API
curl http://localhost:8000/health
```

## ğŸ“Š × ×™×˜×•×¨ ×‘×™×¦×•×¢×™×

```bash
# × ×™×˜×•×¨ GPU ×‘×–××Ÿ ×××ª
watch -n 1 nvidia-smi

# × ×™×˜×•×¨ ×–×™×›×¨×•×Ÿ
htop

# ×œ×•×’×™× ×©×œ ×”××¤×œ×™×§×¦×™×”
tail -f /app/logs/app.log
```

## ğŸ› ï¸ ×˜×™×¤×™× ××ª×§×“××™×

### 1. ×©××™×¨×ª ××•×“×œ×™×
```bash
# ×©××•×¨ ××•×“×œ×™× ×‘-persistent storage
mkdir -p /workspace/models
export TORCH_HOME=/workspace/models
```

### 2. Batch Processing
```python
# ×¢×‘×“ ××¡×¤×¨ ×©×™×¨×™× ×‘××§×‘×™×œ
# (×–×”×™×¨ ××–×™×›×¨×•×Ÿ GPU!)
async def process_multiple_songs(files):
    tasks = [separate_audio(f) for f in files[:3]]  # ××§×¡ 3 ×‘××§×‘×™×œ
    return await asyncio.gather(*tasks)
```

### 3. Auto-shutdown
```bash
# ×”×’×“×¨ auto-shutdown ×œ×—×¡×›×•×Ÿ ×‘×¢×œ×•×™×•×ª
echo "shutdown -h +60" | at now  # ×›×™×‘×•×™ ××—×¨×™ ×©×¢×”
```

## ğŸš¨ ×‘×¢×™×•×ª × ×¤×•×¦×•×ª

### Out of Memory
```bash
# ×”×§×˜×Ÿ segment size
--segment 4  # ×‘××§×•× 16

# ××• ×”×§×˜×Ÿ batch size
--shifts 2   # ×‘××§×•× 10
```

### Connection Timeout
```bash
# ×”×’×“×œ timeout ×‘-frontend
const API_TIMEOUT = 30 * 60 * 1000;  // 30 ×“×§×•×ª
```

### Slow Performance
```bash
# ×‘×“×•×§ ×©GPU ×‘×©×™××•×©
nvidia-smi

# ×‘×“×•×§ ×©×œ× ×¨×¥ ×¢×œ CPU
export CUDA_VISIBLE_DEVICES=0
```

## ğŸ¯ ×”××œ×¦×•×ª

1. **×”×ª×—×œ ×¢× RTX 3090** - ×™×—×¡ ××—×™×¨/×‘×™×¦×•×¢×™× ×”×˜×•×‘ ×‘×™×•×ª×¨
2. **×”×©×ª××© ×‘-Spot Instances** - ×¢×“ 50% ×—×™×¡×›×•×Ÿ
3. **×©××•×¨ ××•×“×œ×™×** - ×”×•×¨×“×” ×—×“-×¤×¢××™×ª ×©×œ 2GB
4. **× ×˜×¨ ×¢×œ×•×™×•×ª** - ×”×’×“×¨ alerts ×‘-RunPod
5. **×‘×“×•×§ logs** - ×œ××‘×—×•×Ÿ ×‘×¢×™×•×ª ××”×¨

---

**××•×›×Ÿ ×œ×¢×™×‘×•×“ ××§×¦×•×¢×™ ×©×œ ×©×™×¨×™× ×¢× GPU! ğŸš€**
